{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Justin', 'Trudeau', 'was'], ['Trudeau', 'was', 'the'], ['was', 'the', 'very'], ['the', 'very', 'picture'], ['very', 'picture', 'of'], ['picture', 'of', 'Mr.'], ['of', 'Mr.', 'Congeniality,'], ['Mr.', 'Congeniality,', 'positive'], ['Congeniality,', 'positive', 'and'], ['positive', 'and', 'alert.'], ['and', 'alert.', 'Donald'], ['alert.', 'Donald', 'Trump'], ['Donald', 'Trump', 'looked'], ['Trump', 'looked', 'as'], ['looked', 'as', 'if'], ['as', 'if', 'he'], ['if', 'he', 'was'], ['he', 'was', 'struggling'], ['was', 'struggling', 'to'], ['struggling', 'to', 'stay'], ['to', 'stay', 'awake.'], ['stay', 'awake.', 'You'], ['awake.', 'You', \"couldn't\"], ['You', \"couldn't\", 'blame'], [\"couldn't\", 'blame', 'him.'], ['blame', 'him.', 'Their'], ['him.', 'Their', 'news'], ['Their', 'news', 'conference'], ['news', 'conference', 'was'], ['conference', 'was', 'a'], ['was', 'a', 'snoozer.'], ['a', 'snoozer.', 'Which'], ['snoozer.', 'Which', 'means'], ['Which', 'means', 'that'], ['means', 'that', 'it'], ['that', 'it', 'was'], ['it', 'was', 'a'], ['was', 'a', 'resounding'], ['a', 'resounding', 'triumph.']]\n"
     ]
    }
   ],
   "source": [
    "# generate N-Grams\n",
    "import unidecode\n",
    "\n",
    "def generate_n_grams(text, n):\n",
    "    word_lst = [str(unidecode.unidecode(w.decode('utf-8'))) for w in text.split()]\n",
    "    ngram_lst = []\n",
    "    for i in range(len(word_lst)-n+1):\n",
    "        ngram_lst.append(word_lst[i:i+n])\n",
    "        \n",
    "    return ngram_lst\n",
    "\n",
    "\n",
    "test_sent = \"\"\"\n",
    "            Justin Trudeau was the very picture of Mr. Congeniality, positive and alert. \n",
    "            Donald Trump looked as if he was struggling to stay awake. \n",
    "            You couldn’t blame him. Their news conference was a snoozer. \n",
    "            Which means that it was a resounding triumph.\n",
    "            \"\"\"\n",
    "\n",
    "print(generate_n_grams(test_sent, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06603622  0.06603622  0.06603622  0.06603622  0.06603622  0.06603622\n",
      "   0.06603622  0.06603622  0.06603622  0.06603622  0.06603622  0.06603622\n",
      "   0.26786777  0.19843566  0.26786777  0.26786777  0.26786777  0.26786777\n",
      "   0.06603622  0.26786777  0.26786777  0.19843566  0.26786777  0.26786777\n",
      "   0.22725259  0.19843566  0.06603622  0.26786777  0.22725259]]\n",
      "*****************************************\n",
      "  (1, 0)\t0.296800509742\n",
      "  (1, 1)\t0.296800509742\n",
      "  (1, 20)\t0.296800509742\n",
      "  (1, 6)\t0.296800509742\n",
      "  (1, 16)\t0.296800509742\n",
      "  (1, 18)\t0.296800509742\n",
      "  (1, 19)\t0.296800509742\n",
      "  (1, 32)\t0.296800509742\n",
      "  (1, 26)\t0.296800509742\n",
      "  (1, 33)\t0.176079617178\n",
      "  (1, 30)\t0.296800509742\n",
      "  (1, 13)\t0.296800509742\n",
      "  (2, 3)\t0.31080556236\n",
      "  (2, 23)\t0.31080556236\n",
      "  (2, 28)\t0.31080556236\n",
      "  (2, 24)\t0.31080556236\n",
      "  (2, 9)\t0.31080556236\n",
      "  (2, 11)\t0.31080556236\n",
      "  (2, 2)\t0.31080556236\n",
      "  (2, 14)\t0.31080556236\n",
      "  (2, 31)\t0.31080556236\n",
      "  (2, 8)\t0.31080556236\n",
      "  (2, 33)\t0.184388242745\n",
      "  (3, 22)\t0.346023735584\n",
      "  (3, 5)\t0.346023735584\n",
      "  (3, 17)\t0.346023735584\n",
      "  (3, 27)\t0.346023735584\n",
      "  (3, 10)\t0.346023735584\n",
      "  (3, 4)\t0.346023735584\n",
      "  (3, 7)\t0.346023735584\n",
      "  (3, 35)\t0.346023735584\n",
      "  (3, 33)\t0.205281746143\n",
      "  (4, 29)\t0.396776776928\n",
      "  (4, 21)\t0.396776776928\n",
      "  (4, 12)\t0.396776776928\n",
      "  (4, 25)\t0.396776776928\n",
      "  (4, 15)\t0.396776776928\n",
      "  (4, 34)\t0.396776776928\n",
      "  (4, 33)\t0.235391452148\n"
     ]
    }
   ],
   "source": [
    "# tf-idf vs tf-isf\n",
    "## TF-IDF formula gives the relative importance of a term in a corpus\n",
    "## ISF denotes uniqueness of that word in all sentences\n",
    "## TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
    "## IDF for a term is defined as logarithm of ratio of total documents available \n",
    "#### in the corpus and number of documents containing the term T.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def tf(word, doc):     \n",
    "    count = doc.count(word)     \n",
    "    total = len(doc)\n",
    "    tf_score = count / float(total)     \n",
    "    return tf_score\n",
    "\n",
    "def n_containing(word, docs):\n",
    "    count = 0     \n",
    "    for doc in docs:         \n",
    "        if doc.count(word) > 0:\n",
    "            count += 1     \n",
    "    return count\n",
    "\n",
    "def isf(word, docs):\n",
    "    doc_count = n_containing(word, docs)     \n",
    "    ratio = len(docs) / float(1 + doc_count )     \n",
    "    return math.log(ratio)\n",
    "\n",
    "def tfisf(word, doc, docs):     \n",
    "    tf_score = tf(word, doc)     \n",
    "    isf_score = isf(word, docs)     \n",
    "    return tf_score * isf_score\n",
    "\n",
    "def compute_tfisf_scores(sentences):\n",
    "    tfisf_scores = []     \n",
    "    for sent in sentences:         \n",
    "        sentence_score = 0         \n",
    "        for word in sent:\n",
    "            sentence_score += tfisf(word,sent,sentences)         \n",
    "        sentence_score /= float(len(sent))         \n",
    "        tfisf_scores.append(sentence_score)     \n",
    "    return normalize([tfisf_scores])  # when there is .reshape(-1, 1) warning, just add a [] outside of the matrix\n",
    "\n",
    "\n",
    "# tf-isf\n",
    "print(compute_tfisf_scores(test_sent[1:30]))\n",
    "\n",
    "print(\"*****************************************\")\n",
    "\n",
    "# tf-idf\n",
    "tf_idf_vector = TfidfVectorizer()\n",
    "print(tf_idf_vector.fit_transform(test_sent.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of course\n",
      "of course\n",
      "of course\n",
      "of course\n",
      "0.333333333333\n"
     ]
    }
   ],
   "source": [
    "# TEXT CLASSIFICATION - Naive Bayesian\n",
    "## when using Naive Bayesian, it's better for each class to contain similar amount of input\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "\n",
    "training_corpus = [\n",
    "                     (\"Does Emmanuel like ice-cream mochi?\", \"food\"),\n",
    "                     (\"Is Emmanuel feeling sad, or mad or happy?\", \"emotion\"),\n",
    "                     (\"Does Emmanuel miss me?\", \"of course\"),\n",
    "                     (\"Strawberry ice-cream tastes better than lavanda flavor.\", \"of course\"),\n",
    "                     (\"Mango ice-cream mochi tastes better than strawberry mochi.\", \"food\"),\n",
    "                     (\"Avacador is the best fruit!\", \"food\"),\n",
    "                     (\"Would Emmanuel like the dessert from my hometown?\", \"food\"),\n",
    "                     (\"My homwtown has the best desert and vegetables and many other things!\", \"food\"),\n",
    "                     (\"Emmanuel is super lovely and sweet!\", \"emotion\"),\n",
    "                     (\"The weather recently is very weird\", \"of course\"),\n",
    "                     (\"Nothing can be as weird as so much work load....\", \"emotion\"),\n",
    "                     (\"Human beings are so complex, you can never imagine what other people will do\", \"emotion\"),\n",
    "                     (\"It is so good to work with people with mild personality, although they are rare...\", \"emotion\"),\n",
    "                     (\"cats like play games\", \"of course\"),\n",
    "                     (\"It is easy to get cold in Spring\", \"of course\")\n",
    "                  ]\n",
    "\n",
    "testing_corpus = [\n",
    "                   (\"Crepe with fruits or cream or chocolate is too sweet, with avacado and bacon is more acceptable\", \"food\"),\n",
    "                   (\"Eating too much weet food makes people get depressed\", \"of course\"),\n",
    "                   (\"Living in a city like Vancouver, it is difficult not to feel depressed\", \"emotion\"),\n",
    "                   (\"Montreal is pretty but super boring\", \"of course\"),\n",
    "                   (\"The best food is home made food\", \"food\"),\n",
    "                   (\"I miss Emmanuel....\", \"emotion\")\n",
    "                 ]\n",
    "\n",
    "# Doesen't work very well here even when the number of classes are the same\n",
    "model = NBC(training_corpus)\n",
    "new_input1 = \"apple ice-cream sounds weird....\"\n",
    "new_input2 = \"pineapple pie is good, but blueberry pie may be better\"\n",
    "new_input3 = \"American food is not food\"\n",
    "new_input4 = \"Without Emmanuel, not that happy any more....\"\n",
    "print(model.classify(new_input1))\n",
    "print(model.classify(new_input2))\n",
    "print(model.classify(new_input3))\n",
    "print(model.classify(new_input4))\n",
    "\n",
    "print(model.accuracy(testing_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emotion' 'of course' 'emotion' 'emotion' 'food' 'emotion']\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    emotion       0.50      1.00      0.67         2\n",
      "       food       1.00      0.50      0.67         2\n",
      "  of course       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.83      0.67      0.67         6\n",
      "\n",
      "Mean Accuracy:  0.666666666667\n"
     ]
    }
   ],
   "source": [
    "# TEXT CLASSIFICATION - SVM\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "\n",
    "training_features = []\n",
    "training_labels = []\n",
    "\n",
    "for r in training_corpus:\n",
    "    training_features.append(r[0])\n",
    "    training_labels.append(r[1])\n",
    "    \n",
    "testing_features = []\n",
    "testing_labels = []\n",
    "\n",
    "for r in testing_corpus:\n",
    "    testing_features.append(r[0])\n",
    "    testing_labels.append(r[1])\n",
    "    \n",
    "# create feature vector\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "# vectorize training data\n",
    "train_vectors = vectorizer.fit_transform(training_features)\n",
    "# vectorize testing data\n",
    "test_vectors = vectorizer.transform(testing_features)\n",
    "\n",
    "# classification with SVM\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "svm_model.fit(train_vectors, training_labels)\n",
    "prediction = svm_model.predict(test_vectors)\n",
    "print(prediction)\n",
    "print (classification_report(testing_labels, prediction))\n",
    "print \"Mean Accuracy: \", svm_model.score(test_vectors, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "11\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# TEXT MATCHING - Levenshtein Distance\n",
    "from Levenshtein import distance\n",
    "\n",
    "s1 = \"Cherry ice-cream is amazing!\"\n",
    "s2 = \"cherry ice-cream is amazing\"\n",
    "s3 = \"cherry ice-cream\"\n",
    "s4 = \"Cherry ice-cream\"\n",
    "print distance(s1, s2)\n",
    "print distance(s2, s3)\n",
    "print distance(s3, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E540\n",
      "I540\n",
      "J136\n",
      "J136\n",
      "['AMNL', None]\n",
      "['AMNL', None]\n",
      "['JPTR', 'APTR']\n",
      "['JPTR', 'APTR']\n",
      "ENANAL\n",
      "INANAL\n",
      "JAPATAR\n",
      "JAPATAR\n"
     ]
    }
   ],
   "source": [
    "# TEXT MATCHING - Phonetic Matching, match words that are phonetically similar\n",
    "import fuzzy\n",
    "\n",
    "soundex = fuzzy.Soundex(4)\n",
    "print soundex('Emmanuel')\n",
    "print soundex('Immanuel')\n",
    "print soundex('Jupyter')\n",
    "print soundex('Jupiter')\n",
    "\n",
    "dmeta = fuzzy.DMetaphone()\n",
    "print dmeta('Emmanuel')\n",
    "print dmeta('Immanuel')\n",
    "print dmeta('Jupyter')\n",
    "print dmeta('Jupiter')\n",
    "\n",
    "print fuzzy.nysiis('Emmanuel')\n",
    "print fuzzy.nysiis('Immanuel')\n",
    "print fuzzy.nysiis('Jupyter')\n",
    "print fuzzy.nysiis('Jupiter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For Phonetic Matching, python Fuzzy contains Soundex, DMetaphone and nysiis. \n",
    "It seems that when it comes to similar pronuncation but different spelling, DMetaphone works better. \n",
    "But if you care about both spelling and pronouncation differences, Soundex and nysiis maybe better\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668153104781 0.790569415042\n",
      "28\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# TEXT MATCHING - Cosine Similarity\n",
    "import math\n",
    "from collections import Counter\n",
    "from Levenshtein import distance\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "text1 = \"I miss all the food in my hometown....\"\n",
    "text2 = \"food in my hometown, I miss them....\"\n",
    "text3 = \"I miss all the food\"\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "vector3 = text_to_vector(text3) \n",
    "cosine1 = get_cosine(vector1, vector2)\n",
    "cosine2 = get_cosine(vector1, vector3)\n",
    "print cosine1, cosine2\n",
    "print distance(text1, text2)\n",
    "print distance(text1, text3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
